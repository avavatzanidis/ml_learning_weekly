{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f71d0b6",
      "metadata": {
        "id": "1f71d0b6"
      },
      "source": [
        "# Logistic Regression\n",
        "Categorical data - Classification issue on unbounded values (0 or 1)\n",
        "</br>Requires the use of a *sigmoid activation function*\n",
        "</br>Sigmoid function maps between 0 and 1: $s(z) = \\frac{1}{1 + e^(-z)}$, where $z = w^tx + b$\n",
        "\n",
        "- $z$: linear combination\n",
        "- $w$: weight vector\n",
        "- $b$: bias\n",
        "- $w^tx$: dot product of the weights and features\n",
        "- $s(z) or \\hat{y}$: predicted probability\n",
        "\n",
        "Rule:\n",
        "- If s(z) >= 0.5 then -> 1\n",
        "- Else, predict 0\n",
        "\n",
        "## Cost function\n",
        "\n",
        "Unlike Linear Regression, we cannot use mean squared error ‚Äî applying it to sigmoid outputs leads to a **non-convex cost surface**, making optimization unstable.\n",
        "\n",
        "Instead, logistic regression uses **binary cross-entropy loss** (also known as **log loss**):\n",
        "\n",
        "**Loss =**  \n",
        "`-(1/m) * Œ£ [y(i) * log(≈∑(i)) + (1 - y(i)) * log(1 - ≈∑(i))]`\n",
        "\n",
        "Where:  \n",
        "- `y(i)` is the true label (0 or 1)  \n",
        "- `≈∑(i)` is the predicted probability from the sigmoid function\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Intuition\n",
        "\n",
        "- The loss measures how far off the predicted probability `≈∑` is from the actual label `y`.\n",
        "- If the **true label is 1**, only the term `y * log(≈∑)` matters:\n",
        "  - If `≈∑` is close to 1 ‚Üí **low loss**\n",
        "  - If `≈∑` is close to 0 ‚Üí **high loss**\n",
        "- If the **true label is 0**, only the term `(1 - y) * log(1 - ≈∑)` matters:\n",
        "  - If `≈∑` is close to 0 ‚Üí **low loss**\n",
        "  - If `≈∑` is close to 1 ‚Üí **high loss**\n",
        "- This means **confident, wrong predictions are penalized heavily**, which encourages the model to output well-calibrated probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Why is this loss function used?\n",
        "\n",
        "- It follows from **maximum likelihood estimation** ‚Äî we're maximizing the probability that our model assigns to the correct labels.\n",
        "- It is **convex** when used with a sigmoid, so gradient descent can reliably find a **global minimum**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pprint\n",
        "import pickle"
      ],
      "metadata": {
        "id": "cNmVrjfswcx2"
      },
      "id": "cNmVrjfswcx2",
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}